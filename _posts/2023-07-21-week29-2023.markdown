---
layout: post
title:  "Week #29 2023 - Hugging Face and The AI Democratization"
date:   2023-07-21 20:00:00 +0700
---

## Hugging Face and The AI Democratization

Hugging Face is a key player in the democratization of artificial intelligence (AI), particularly in the field of natural language processing (NLP). The company contributes to the AI community by providing open-source tools and resources, fostering an environment of collective progress and learning.

At the heart of Hugging Face's mission lies the Hugging Face Hub—a repository for models, datasets, and spaces. The Hub hosts a vast collection of pre-trained models and datasets, while also offering a platform, called Spaces, for developers to create, host, and share machine learning projects. It simplifies the process of AI implementation and facilitates collaborative learning and development.

One of Hugging Face's significant contributions is the Transformers library, a user-friendly interface to state-of-the-art transformer models. The library includes features like 'Pipelines' that streamline the use of models for various NLP tasks like text classification, named entity recognition, question answering, text generation, translation, and summarization.

__AI advancement and community collaboration__

Artificial intelligence (AI) has rapidly become a transformational force in almost every sector of society. These advancements are not emerging from a vacuum but are the fruit of collective efforts in the global scientific community. As with many technological advances throughout history, the narrative is not one of lone pioneers but a tale of collaboration and shared learning.

A defining characteristic of this shared journey has been the rise of open-source projects. In the context of AI, open source allows researchers and developers worldwide to contribute to a project, enhancing its effectiveness and expanding its applicability. It offers unprecedented transparency, enabling anyone to scrutinize, learn from, and improve upon existing models and methodologies.

The AI journey is indeed not a solitary endeavor but a community trek where every contribution, big or small, pushes the boundaries of what we once thought impossible.

__Hugging Face and AI Democratization__

Hugging Face, aptly named after the standard emoji symbolizing a warm, friendly hug, was founded to make AI more approachable, less intimidating, and more community-oriented. It began its journey as an AI chatbot company, but its shift towards creating tools and libraries for NLP marked the turning point.

Today, they are renowned for their Transformers library, which has become a go-to resource for anyone dealing with NLP tasks. This library houses state-of-the-art models and is utilized by various research institutions, tech giants, and AI enthusiasts.

Hugging Face has built a community where researchers and developers worldwide can share their pre-trained models, engage in discussions, collaborate on projects, and contribute to the ever-growing collective AI knowledge pool. The approach lowers the entry barriers to AI and thus contributes to the democratization of the field.  

__The Hugging Face Hub__

In the heart of Hugging Face's mission to democratize AI lies the Hugging Face Hub—an innovative platform that acts as a central repository for models, datasets, and spaces. Serving as a shared storage system, it plays a significant role in the AI community by providing easy access to a wealth of AI resources.

**Repositories**

The Hub is built around the concept of repositories, akin to GitHub for code. These repositories store models and datasets and provide version control, allowing developers to iterate on models effectively and efficiently. Every item hosted in a repository comes with its own webpage that details its use, provides code examples, and offers valuable insights on implementation.

**Models**

The models hosted on the Hub are its most famous feature. The platform provides a wealth of pre-trained models, encompassing a broad range of NLP tasks in various languages. These models, many of which users contribute, range from the BERT and GPT families to many other transformer architectures. Anyone can freely use these models, fine-tune them, or contribute their own, fostering a culture of collaborative learning and development.

**Datasets**

Datasets are another critical component of the Hugging Face Hub. These are used to train and fine-tune the models. The Hub hosts a plethora of datasets spanning multiple languages and addressing diverse tasks. Users can explore datasets, understand their structure, and directly use them for training models. This access to quality data significantly simplifies the process of training AI models.

**Spaces**

One of the more recent additions to the Hugging Face Hub is Spaces, an environment allowing developers to create, host, and share machine-learning projects. In Spaces, users can build web-based, end-to-end machine learning applications, often leveraging the models and datasets available on the Hub. Spaces opens up a world of possibilities for developers to experiment, learn, and share their work with the broader AI community.  

__Transformers__

Transformers are a technology that has revolutionized how machines understand and generate human language, leading to impressive improvements in various tasks. Hugging Face, in its commitment to democratizing AI, has played a significant role in this revolution with its groundbreaking Transformers library.

Before the advent of Transformers, NLP tasks largely relied on models like recurrent neural networks (RNN) and long short-term memory (LSTM) networks. While these models marked considerable advancements in NLP, they had limitations. Their sequential nature meant they were time-consuming to train, and they struggled with long sequences, affecting their ability to understand context over long texts.

Transformers were introduced to overcome these challenges. Unlike RNNs and LSTMs that handle data sequentially, Transformers process input data in parallel, significantly improving efficiency. The key to their success is an attention mechanism known as 'Self-Attention' or 'Scaled Dot-Product Attention.' This mechanism allows models to weigh the significance of words in a sentence relative to all other words, enabling a deeper understanding of the context.   

__*Transformers Library*__

Hugging Face's Transformers library has been instrumental in bringing this transformative technology to the hands of researchers, developers, and hobbyists. This library offers a unified, simple-to-use interface to an extensive collection of Transformer models. The library's design enables rapid prototyping, making it a preferred choice for many NLP tasks.

A standout feature of the Transformers library is 'Pipelines.' Pipelines streamline the process of using a model to make predictions. They wrap around a model and provide a simple API for various NLP tasks, like text generation, translation, summarization, and more. Whether you want to classify a text or generate a new one, pipelines help you with just a few lines of code.

__Common NLP Tasks__

Let's explore some NLP tasks commonly found in the transformer's utilization.

__*Text Classification*__

Text classification involves assigning predefined categories to a given text. It's often used in sentiment analysis, where text is classified as positive, negative, or neutral. For example, a business can analyze customer reviews to understand their sentiment toward their products or services.

__*Named Entity Recognition (NER)*__

NER involves identifying and classifying named entities in text into predefined categories such as person names, organizations, locations, date expressions, percentages, etc. For instance, in the sentence, "Apple was founded by Steve Jobs in Cupertino," NER would identify "Apple" as an organization, "Steve Jobs" as a person, and "Cupertino" as a location.

__*Text Generation*__

Text generation models are capable of generating human-like text given some input. These models are used in various applications, from writing assistance tools to chatbots. For instance, given the prompt "Once upon a time," a text generation model could generate a complete story.

__*Translation*__

Translation models are designed to translate text from one language to another. For example, a model could translate a paragraph written in English into French, German, Spanish, or any other supported language.

__*Summarization*__

Summarization models are tasked with creating a concise summary of a longer text. For example, a news organization might use a summarization model to generate summaries of lengthy articles for readers who are short on time.

Hugging Face's Transformers library provides an extensive collection of pre-trained models for these tasks, enabling developers and researchers to utilize state-of-the-art NLP tools easily and efficiently.

## Tech News

![memo](/assets/images/memo16.png) *[Introducing Llama 2](https://ai.meta.com/llama/)*

**Brain**: "Meta just announced their latest open-source LLM model, which allows commercial use. Llama 2 comes in 3 different sizes, 7B, 13B, and 70B. The performance of the 70B model is said to be roughly tied to the GPT-3.5-0301 model, though it still has weak performance on coding abilities. Nevertheless, this open-source alternative surely gives some excitement, especially those concerned about privacy or the cost of paid LLM model."

![memo](/assets/images/memo16.png) *[What is Expert System?](https://www.guru99.com/expert-systems-with-applications.html)*

**Dika**: "Expert Systems are computer-based decision-making systems that utilize facts and heuristics to solve complex problems. They represent the highest level of expertise and can handle challenges in specific domains. The components include User Interface, Inference Engine, and Knowledge Base. Advantages include improved decision quality, cost reduction, and consistent answers, while limitations involve creativity and maintenance costs. Applications span various fields like medicine, finance, and scheduling."

![memo](/assets/images/memo16.png) *[GitHub's Copilot Chat AI feature is now available in public beta](https://www.theverge.com/2023/7/20/23801498/github-copilot-x-chat-code-chatbot-public-beta)*

**Rizqun**: "GitHub has announced that its new Copilot Chat feature is now available as a limited public beta for enterprise companies and organizations. According to GitHub, Copilot Chat is contextually aware of the code being typed into the code editor and any error messages, which should help provide the most relevant support within a developer-specific environment."

![memo](/assets/images/memo16.png) *[New ChatGPT rival, Claude 2, launches for open beta testing](https://arstechnica.com/information-technology/2023/07/new-chatgpt-rival-claude-2-launches-for-open-beta-testing/)*

**Yoga**: "Anthropic introduces Claude 2, a powerful language model like ChatGPT. It excels in coding, math, and reasoning, scoring well in exams and demonstrating improved proficiency. Claude 2 can analyze long documents and has reduced harmful outputs. Available for free on a beta website and as a commercial API, it is now used by companies like Jasper and Sourcegraph. Remember to use AI models responsibly and avoid critical health and well-being matters."

![memo](/assets/images/memo16.png) *[Introducing TypeChat](https://microsoft.github.io/TypeChat/blog/introducing-typechat/)*

**Frandi**: "Microsoft just released an open-source library that can integrate AI into a web application or web APIs. It uses the already well-known language in the web development world: TypeScript. This approach is expected to accelerate AI adoption in the traditional web development ecosystem."
